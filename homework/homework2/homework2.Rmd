---
title: "Homework 2"
author: "`r paste0('PSTAT 115, ', lubridate::year(Sys.Date()))`"
date: "__Due on February 6, 2023 at 11:59 pm__"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
library(testthat)
eval <- TRUE
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE,
                      eval = eval,
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center')
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')
r = function(x, digits=2){ round(x, digits=digits) }
library(tidyverse)
library(reshape2)
```

__Note:__ If you are working with a partner, please submit only one homework per group with both names and whether you are taking the course for graduate credit or not.  Submit your Rmarkdown (.Rmd) and the compiled pdf on Gauchospace.

## 1. Trend in Same-sex Marriage

A 2017 Pew Research survey found that 10.2% of LGBT adults in the U.S. were married to a same-sex spouse. Now it’s the 2020s, and Bayard guesses that $\pi$, the percent of LGBT adults in the U.S. who are married to a same-sex spouse, has most likely increased to about 15% but could reasonably range from 10% to 25%.

**1a.** Identify a Beta model that reflects Bayard’s prior ideas about $\pi$ by specifying the parameters of the Beta, $\alpha$ and $\beta$.

```{r tags=c()}
alpha <- NULL # YOUR CODE HERE
beta <- NULL # YOUR CODE HERE
```
```{r}
. = ottr::check("tests/q1a.R")
```

**1b.** Bayard wants to update his prior, so he randomly selects 90 US LGBT adults and 30 of them are married to a same-sex partner. What is the posterior model for $\pi$?

```{r tags=c()}
posterior_alpha <- NULL # YOUR CODE HERE
posterior_beta <- NULL # YOUR CODE HERE
```

**1c.** Use R to compute the posterior mean and standard deviation of $\pi$.

```{r tags=c()}
posterior_mean <- NULL # YOUR CODE HERE
posterior_sd <- NULL # YOUR CODE HERE

print(sprintf("The posterior mean is %f", posterior_mean))
print(sprintf("The posterior sd is %f", posterior_sd))
```

**1d.** Does the posterior model more closely reflect the prior information or the data? Explain your reasoning. Hint: in the recorded lecture we showed a special way in which we can write the posterior mean in a Beta-Binomial model.  How can this help? Check the lectures notes.

_Type your answer here, replacing this text._

```{r tags=c()}
# YOUR CODE HERE
```

## 2. Cancer Research in Laboratory Mice

A laboratory is estimating the rate of tumorigenesis (the formation of tumors) in two strains of mice, A and B.  They have tumor count data for 10 mice in strain A and 13 mice in strain B.  Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. Assuming a Poisson sampling distribution for each group with rates $\theta_A$ and $\theta_B$. Based on previous research you settle on the following prior distribution:

$$ \theta_A \sim \text{gamma}(120, 10),\ \theta_B\sim\text{gamma}(12, 1)$$ 

**2a.** Before seeing any data, which group do you expect to have a higher average incidence of cancer?  Which group are you more certain about a priori? You answers should be based on the priors specified above.

_Type your answer here, replacing this text._

**2b.**  After you the complete of the experiment, you  observe the following tumor counts for the two populations: 

$$y_A = (12,9,12,14,13,13,15,8,15,6)$$
$$y_B = (11,11,10,9,9,8,7,10,6,8,8,9,7)$$
    
Compute the posterior parameters, posterior means, posterior variances and 95% quantile-based credible intervals for $\theta_A$ and $\theta_B$.  Same them in the appropriate variables in the code cell below.  You do not need to show your work, but you cannot get partial credit unless you do show work.

```{r summary_stats, echo=FALSE}
yA <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB <- c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)

# Prior parameters here
alpha_A = NULL # YOUR CODE HERE
beta_A = NULL # YOUR CODE HERE

alpha_B = NULL # YOUR CODE HERE
beta_B = NULL # YOUR CODE HERE

# Posterior parameters here
alpha_A_posterior = NULL # YOUR CODE HERE
beta_A_posterior = NULL # YOUR CODE HERE

alpha_B_posterior = NULL # YOUR CODE HERE
beta_B_posterior = NULL # YOUR CODE HERE
            
## Posterior mean and variance for each group        
A_post_mean <- NULL # YOUR CODE HERE
A_post_var <- NULL # YOUR CODE HERE

# Posterior quantiles for each group
B_post_mean <- NULL # YOUR CODE HERE
B_post_var <- NULL # YOUR CODE HERE

print(paste0("Posterior mean of theta_A ", round(A_post_mean, 2)))
print(paste0("Posterior variance of theta_A ", round(A_post_var, 2)))
print(paste0("Posterior mean of theta_B ", round(B_post_mean, 2)))
print(paste0("Posterior variance of theta_B ", round(B_post_var, 2)))

# Posterior quantiles
alpha_A_quantile <- NULL # YOUR CODE HERE
alpha_B_quantile <- NULL # YOUR CODE HERE

print(paste0("Posterior 95% quantile for theta_A is [", round(alpha_A_quantile[1],2), ", ", round(alpha_A_quantile[2], 2), "]"))
print(paste0("Posterior 95% quantile for theta_B is [", round(alpha_B_quantile[1],2), ", ", round(alpha_B_quantile[2], 2), "]"))
```
```{r}
. = ottr::check("tests/q2b.R")
```

**2c.** Compute and plot the posterior expectation of $\theta_B$ given $y_B$ under the prior distribution  $\text{gamma}(12\times n_0, n_0)$ for each value of $n_0 \in \{1,2,...,50\}$. As a reminder, $n_0$ can be thought of as the number of prior observations (or pseudo-counts).  



```{r tags=c()}

# YOUR CODE HERE

posterior_means = NULL # YOUR CODE HERE

# YOUR CODE HERE
```
```{r}
. = ottr::check("tests/q2c.R")
```

**2d.** Should knowledge about population A tell us anything about population B? Discuss whether or not it makes sense to have $p(\theta_A, \theta_B) = p(\theta_A) \times p(\theta_B)$.  

_Type your answer here, replacing this text._

\vspace{.2in}

## 3. Soccer World cup
Let $\lambda$ be the expected number of goals scored in a Women’s World Cup game. We’ll analyze $\lambda$ by the following a $Y_i$ is the observed number of goals scored in a sample of World Cup games:
\[Y_i | \lambda  \stackrel{ind}{\sim} \text{Pois}(\lambda) \]

You and your friend argue about a more reasonable prior for $\lambda$.    You think that $p_1(\lambda)$ with a $\text{gamma}(8, 2)$ density is a reasonable prior. Your friend thinks that $p_2(\lambda)$ with a $\text{gamma}(2, 1)$ density is a reasonable prior distribution.  Consider the case in which each of you are equally credible in your prior assessments and so you combine your prior distributions into a mixture prior with equal weights: $p(\lambda) = 0.5 * p_1(\lambda) + 0.5 * p_2(\lambda)$.

**3a.** Which of you thinks more goals will be scored on average? Which of you is more confident in that assessment a priori?

_Type your answer here, replacing this text._

**3b.** Plot the combined prior density, $p(\lambda)$, that you and your friend have created.

```{r tags=c()}
# YOUR CODE HERE
```

**3c.** Why might the Poisson model be a reasonable model for our data $Y_i$? In what ways might this model for $Y_i$ be too simple?

**3c.** The `wwc_2019_matches` data in the *fivethirtyeight* package includes the number of goals scored by the two teams in each 2019 Women’s World Cup match. Create a histogram of the number of goals scored per game.  What is the maximum likelihood estimate for the expected number of goals scored in a game? You do not need to show your work for computing the MLE.

```{r}
library(fivethirtyeight)
data("wwc_2019_matches")
wwc_2019_matches <- wwc_2019_matches %>% 
  mutate(total_goals = score1 + score2)

## This is your y_i
total_goals <- wwc_2019_matches$total_goals
```


```{r tags=c()}
soccer_mle <- NULL # YOUR CODE HERE
```
**3d.** You decide to stick with your original prior, the Gamma(8, 2), your friends prior or the mixture prior.  Write the posterior distribution distribution and add a vertical line at the MLE and the prior mean. Based on the plot above would you say that the prior had a large impact on conclusions or only a small one? Reference pseudo-counts and the proposed prior to argue why it makes sense that the prior did or did not have a big effect.

    
